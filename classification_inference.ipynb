{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Sentence Classifyer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fields: List[str], files: Dict[str, str], mapping: Dict[str, int]) -> pd.DataFrame:\n",
    "    dataset = {}\n",
    "    \n",
    "    def encode_label(label: str):\n",
    "        return mapping[label]\n",
    "\n",
    "    def load_data(file: str):\n",
    "        df = pd.read_json(file, lines=True)\n",
    "\n",
    "        data = []\n",
    "        for _, i in df.iterrows():\n",
    "            text = \"\"\n",
    "            for field in fields:\n",
    "                if isinstance(i[field], list):\n",
    "                    text += ' '.join(i[field])\n",
    "                elif isinstance(field, str):\n",
    "                    text += i[field]\n",
    "                else:\n",
    "                    raise NotImplemented\n",
    "\n",
    "            data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": encode_label(i[\"tags\"][0])})\n",
    "        return data\n",
    "\n",
    "\n",
    "    for split in list(files.keys()):\n",
    "        dataset[split] = load_data(files[split])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=[\"postText\", \"targetTitle\", \"targetParagraphs\"]\n",
    "dataset = load_dataset(fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text[\"text\"]for text in dataset[\"validation\"]]\n",
    "text = texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multi model prediction\n",
    "checkpoint = \"path to model\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "mapping = {0: 'passage', 1: 'phrase', 2: 'multi'}\n",
    "\n",
    "def predict_one(text, model):\n",
    "    tokenized = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokenized).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return mapping[predicted_class_id]\n",
    "\n",
    "predicted_tag = predict_one(text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs. rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one vs rest prediction\n",
    "checkpoint = {\"passage\": \"path to model\", \"phrase\": \"path to model\", \"multi\": \"path to model\"}\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint[\"passage\"])\n",
    "models = {\n",
    "    \"passage\": AutoModelForSequenceClassification.from_pretrained(checkpoint[\"passage\"], num_labels=2),\n",
    "    \"phrase\": AutoModelForSequenceClassification.from_pretrained(checkpoint[\"phrase\"], num_labels=2),\n",
    "    \"multi\": AutoModelForSequenceClassification.from_pretrained(checkpoint[\"multi\"], num_labels=2)\n",
    "}\n",
    "\n",
    "def predict_one_multi(text, model):\n",
    "    def predict_probability(text, model):\n",
    "        tokenized = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**tokenized).logits\n",
    "        return logits.argmax()\n",
    "\n",
    "\n",
    "    probabilities = {}\n",
    "    for tag_name, model in models.items():\n",
    "        probability = predict_probability(text, model)\n",
    "        probabilities[tag_name] = probability\n",
    "\n",
    "    return max(probabilities, key=probabilities.get)\n",
    "\n",
    "\n",
    "predicted_tag = predict_one_multi(text, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ANLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a963ddbb9d4d2ccdf936f46bc94ee37518f3a93a08eb4602276258409c4ce148"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
